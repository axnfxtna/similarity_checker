{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cc0bbc",
   "metadata": {},
   "source": [
    "--- Query Page 0 ---\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_0 | Score: 57.86%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_9 | Score: 51.06%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_2 | Score: 50.42%\n",
    "Matched PDF: 1-s2.0-S0749596X25000695-main.pdf_page_17 | Score: 49.57%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_1 | Score: 48.21%\n",
    "--- Query Page 1 ---\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_3 | Score: 53.91%\n",
    "Matched PDF: 1-s2.0-S0957417425022948-main.pdf_page_12 | Score: 53.27%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_0 | Score: 49.08%\n",
    "Matched PDF: 1-s2.0-S074756322500216X-main.pdf_page_1 | Score: 48.92%\n",
    "Matched PDF: 1-s2.0-S2772485925000481-main.pdf_page_1 | Score: 47.65%\n",
    "--- Query Page 2 ---\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_3 | Score: 54.33%\n",
    "Matched PDF: 1-s2.0-S0957417425022948-main.pdf_page_12 | Score: 53.25%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_1 | Score: 50.36%\n",
    "Matched PDF: 1-s2.0-S074756322500216X-main.pdf_page_15 | Score: 50.15%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_2 | Score: 48.99%\n",
    "--- Query Page 3 ---\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_9 | Score: 56.68%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_0 | Score: 53.53%\n",
    "Matched PDF: 1-s2.0-S2095177925002230-main.pdf_page_49 | Score: 53.21%\n",
    "Matched PDF: 1-s2.0-S2095177925002230-main.pdf_page_7 | Score: 52.83%\n",
    "Matched PDF: 1-s2.0-S2095177925002230-main.pdf_page_52 | Score: 52.83%\n",
    "--- Query Page 4 ---\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_3 | Score: 60.87%\n",
    "Matched PDF: 1-s2.0-S0957417425022948-main.pdf_page_12 | Score: 50.47%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_1 | Score: 50.02%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_0 | Score: 44.66%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_2 | Score: 43.41%\n",
    "--- Query Page 5 ---\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_8 | Score: 53.49%\n",
    "Matched PDF: 1-s2.0-S0720048X25004449-main.pdf_page_5 | Score: 45.82%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_5 | Score: 45.22%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_17 | Score: 45.18%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_18 | Score: 43.75%\n",
    "--- Query Page 6 ---\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_5 | Score: 56.63%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_9 | Score: 56.26%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_2 | Score: 55.64%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_0 | Score: 55.37%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_1 | Score: 54.49%\n",
    "--- Query Page 7 ---\n",
    "Matched PDF: 1-s2.0-S0749596X25000695-main.pdf_page_15 | Score: 53.27%\n",
    "Matched PDF: 1-s2.0-S2095177925002230-main.pdf_page_62 | Score: 50.64%\n",
    "Matched PDF: 1-s2.0-S0164121225002389-main.pdf_page_11 | Score: 50.41%\n",
    "Matched PDF: 1-s2.0-S0164121225002389-main.pdf_page_12 | Score: 50.13%\n",
    "Matched PDF: 1-s2.0-S2095177925002230-main.pdf_page_49 | Score: 46.22%\n",
    "--- Query Page 8 ---\n",
    "Matched PDF: 1-s2.0-S0749596X25000695-main.pdf_page_15 | Score: 43.99%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_18 | Score: 40.78%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_20 | Score: 40.72%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_17 | Score: 40.10%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_5 | Score: 39.82%\n",
    "--- Query Page 9 ---\n",
    "Matched PDF: 1-s2.0-S0749596X25000695-main.pdf_page_15 | Score: 46.59%\n",
    "Matched PDF: 1-s2.0-S2095177925002230-main.pdf_page_49 | Score: 42.81%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_20 | Score: 42.22%\n",
    "Matched PDF: 1-s2.0-S0164121225002389-main.pdf_page_12 | Score: 41.60%\n",
    "Matched PDF: 1-s2.0-S0164121225002389-main.pdf_page_11 | Score: 41.43%\n",
    "--- Query Page 10 ---\n",
    "Matched PDF: 1-s2.0-S0749596X25000695-main.pdf_page_15 | Score: 48.73%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_20 | Score: 41.98%\n",
    "Matched PDF: 1-s2.0-S0720048X25004449-main.pdf_page_1 | Score: 41.04%\n",
    "Matched PDF: 1-s2.0-S0749596X25000695-main.pdf_page_17 | Score: 41.04%\n",
    "Matched PDF: 1-s2.0-S2095177925002230-main.pdf_page_49 | Score: 40.20%\n",
    "--- Query Page 11 ---\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_8 | Score: 57.43%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_2 | Score: 52.82%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_17 | Score: 51.42%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_5 | Score: 51.33%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_1 | Score: 51.21%\n",
    "--- Query Page 12 ---\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_8 | Score: 56.74%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_17 | Score: 50.87%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_19 | Score: 50.11%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_18 | Score: 49.90%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_20 | Score: 49.58%\n",
    "--- Query Page 13 ---\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_4 | Score: 56.80%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_1 | Score: 55.56%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_9 | Score: 54.25%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_8 | Score: 53.31%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_20 | Score: 51.98%\n",
    "--- Query Page 14 ---\n",
    "Matched PDF: 1-s2.0-S0957417425022948-main.pdf_page_12 | Score: 44.42%\n",
    "Matched PDF: 1-s2.0-S0749596X25000695-main.pdf_page_10 | Score: 43.63%\n",
    "Matched PDF: 1-s2.0-S0749596X25000695-main.pdf_page_17 | Score: 43.29%\n",
    "Matched PDF: 1-s2.0-S1071581925001466-main.pdf_page_13 | Score: 42.74%\n",
    "Matched PDF: 1-s2.0-S0957417425022948-main.pdf_page_3 | Score: 42.31%\n",
    "--- Query Page 15 ---\n",
    "Matched PDF: 1-s2.0-S0749596X25000695-main.pdf_page_15 | Score: 56.80%\n",
    "Matched PDF: 1-s2.0-S0749596X25000695-main.pdf_page_1 | Score: 50.90%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_20 | Score: 47.46%\n",
    "Matched PDF: 1-s2.0-S2949719125000275-main.pdf_page_5 | Score: 45.71%\n",
    "Matched PDF: 1-s2.0-S0957417425022948-main.pdf_page_12 | Score: 45.33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c62afab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pdf_filepath = \"/home/natcha/rag_document/1-s2.0-S0957417425026181-main.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0730d51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle this task. The user wants me to highlight sentences or paragraphs from both the query and the matched PDF that have similar ideas or main points. Then, I need to show those and explain why they are similar.\n",
      "\n",
      "First, I need to look at the query page. The user provided two sections. Let me check the query again. The first part is about the problem with LLMs, like bias and information accuracy. Then, in the matched PDF, there's a section on the same topic. So, the first paragraph from the query about issues with LLMs and the corresponding paragraph from the PDF should be highlighted.\n",
      "\n",
      "Now, the explanation part. The user wants a clear and natural language explanation. So, I need to state that the query's first paragraph and the PDF's corresponding paragraph are similar in that they both discuss the same problems related to LLM-generated text detection. That makes sense because they both focus on the same issue.\n",
      "\n",
      "Wait, but the query's first paragraph is about LLM issues, and the PDF's section is about LLM-generated text detection. So the similarity is in the same topic, not the same problem. That's why the explanation is straightforward. No need to get into details here. Just state that the two parts are similar in the same main topic.\n",
      "</think>\n",
      "\n",
      "1. **Similarity Highlight**:  \n",
      "   - **Query**: The first paragraph discusses issues with LLM-generated text, such as bias, information accuracy, and potential inaccuracies.  \n",
      "   - **Matched PDF**: The corresponding section also addresses similar topics, including bias and information inaccuracies.  \n",
      "\n",
      "2. **Explanation**:  \n",
      "   These parts share the same focus on LLM-generated text detection problems, such as bias in training data (e.g., gender or race) and inaccuracies due to training sources.  \n",
      "\n",
      "3. **Key Reason**:  \n",
      "   Both documents discuss the same core issue: biases in LLM-generated content and the risk of information misrepresentation, which are central to the broader context of LLM-generated text detection.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_page_text(pdf_path, page_number):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    if page_number < len(doc):\n",
    "        text = doc[page_number].get_text(\"text\")\n",
    "    else:\n",
    "        text = \"\"\n",
    "    doc.close()\n",
    "    return text.strip()\n",
    "\n",
    "query_pdf = extract_page_text(\"/home/natcha/rag_document/1-s2.0-S0957417425026181-main.pdf\", 1)      # page 1\n",
    "matched_pdf = extract_page_text(\"/home/natcha/rag_document/dataset/1-s2.0-S2949719125000275-main.pdf\", 3)\n",
    "similarity_percentage = 53.91\n",
    "\n",
    "# Run a prompt on qwen3:0.6b-q4_K_M\n",
    "response = ollama.generate(\n",
    "    model=\"qwen3:0.6b-q4_K_M\",\n",
    "    prompt=f\"\"\"\n",
    "You are analyzing two pieces of text from different PDF pages.\n",
    "Compare the following two PDF page texts and justify why their similarity score is {similarity_percentage}%.  \n",
    "\n",
    "Here is the query page text:\n",
    "---\n",
    "{query_pdf}\n",
    "---\n",
    "\n",
    "Here is the matched database page text:\n",
    "---\n",
    "{matched_pdf}\n",
    "---\n",
    "\n",
    "Tasks:\n",
    "1. Highlight sentences or paragraphs from the query page that have similar information or main idea.  \n",
    "2. Show the corresponding sentences or paragraphs from the matched PDF.  \n",
    "3. Explain briefly why these parts are considered similar in clear, natural language. \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c83d2637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's start by understanding the user's request. They want me to highlight sentences or paragraphs from both the query and the matched PDF, show them, explain why they're similar, and write the explanation in clear, natural language.\n",
      "\n",
      "First, I need to look at both documents. The query page has some content, and the matched PDF also has similar parts. I should find the sentences or paragraphs that are similar. For example, in the query, there's a mention of LLM-generated content, which matches the matched PDF's discussion on the risk of information being inaccuracy. That's a good example.\n",
      "\n",
      "Next, I need to identify the corresponding sentences or paragraphs from the matched PDF. Let me check each document. In the query, there's a section about bias in LLMs, and in the matched PDF, there's a similar part discussing underrepresentation of certain groups. That's a clear similarity.\n",
      "\n",
      "Then, I need to explain why these parts are similar. The query's content is about bias in LLMs, which matches the matched PDF's discussion on underrepresentation. The explanation should be brief, explaining the connection between the two documents.\n",
      "\n",
      "Finally, I need to write the explanation in clear, natural language. Make sure it's concise and addresses why the parts are similar. Let me put this all together step by step.\n",
      "</think>\n",
      "\n",
      "1. **Highlighted Sentences from Query and Matched PDF**:  \n",
      "   - **Query**: \"LLM-generated text might be inaccurate, even if the information is factual (Mitchell et al., 2023; Illia et al., 2023; Lin et al., 2022).\"  \n",
      "   - **Matched PDF**: \"Information accuracy is not guaranteed by LLMs, which can end up misleading users... (Kirchenbauer et al., 2023).\"  \n",
      "\n",
      "2. **Similarity Explanation**:  \n",
      "   These parts discuss the **inaccuracy** of LLM-generated content, a key concern in the field. Both documents highlight that inaccurate information can lead to credibility issues, which is a critical issue in detecting LLM-generated text. The focus on information reliability aligns with the broader problem of bias in the model itself.  \n",
      "\n",
      "3. **Explanations in Clear Language**:  \n",
      "   The query emphasizes the **risk of inaccuracy** in LLM outputs, while the matched PDF discusses the **bias in training data**. This connection shows how the issue of information reliability is part of the larger challenge in detecting LLM-generated text.  \n",
      "\n",
      "4. **Conclusion**:  \n",
      "   These parts highlight the **inaccuracy and bias** in LLM-generated content, reinforcing the need for careful detection to mitigate issues like information misrepresentation.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_page_text(pdf_path, page_number):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    if page_number < len(doc):\n",
    "        text = doc[page_number].get_text(\"text\")\n",
    "    else:\n",
    "        text = \"\"\n",
    "    doc.close()\n",
    "    return text.strip()\n",
    "\n",
    "query_pdf = extract_page_text(query_pdf_filepath, 1)      # page 1\n",
    "matched_pdf = extract_page_text(\"/home/natcha/rag_document/dataset/1-s2.0-S2949719125000275-main.pdf\", 3)\n",
    "similarity_percentage = 53.91\n",
    "\n",
    "# Run a prompt on qwen3:0.6b-q4_K_M\n",
    "response = ollama.generate(\n",
    "    model=\"qwen3:0.6b-q4_K_M\",\n",
    "    prompt=f\"\"\"\n",
    "You are comparing two PDF pages. \n",
    "Your goal is to explain why they are considered {similarity_percentage}% similar.\n",
    "\n",
    "Here is the query page text:\n",
    "---\n",
    "{query_pdf}\n",
    "---\n",
    "\n",
    "Here is the matched database page text:\n",
    "---\n",
    "{matched_pdf}\n",
    "---\n",
    "\n",
    "Tasks:\n",
    "1. Highlight sentences or paragraphs from the query page that are similar.  \n",
    "2. Show the corresponding sentences or paragraphs from the matched PDF.  \n",
    "3. Explain briefly why these parts are considered similar.  \n",
    "4. Write the explanation in clear, natural language.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99af5ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Highlighted Sentences from Query and Matched PDF**:  \n",
      "   - **Query**: \"LLM-generated text might be inaccurate, even if the information is factual (Mitchell et al., 2023; Illia et al., 2023; Lin et al., 2022).\"  \n",
      "   - **Matched PDF**: \"Information accuracy is not guaranteed by LLMs, which can end up misleading users... (Kirchenbauer et al., 2023).\"  \n",
      "\n",
      "2. **Similarity Explanation**:  \n",
      "   These parts discuss the **inaccuracy** of LLM-generated content, a key concern in the field. Both documents highlight that inaccurate information can lead to credibility issues, which is a critical issue in detecting LLM-generated text. The focus on information reliability aligns with the broader problem of bias in the model itself.  \n",
      "\n",
      "3. **Explanations in Clear Language**:  \n",
      "   The query emphasizes the **risk of inaccuracy** in LLM outputs, while the matched PDF discusses the **bias in training data**. This connection shows how the issue of information reliability is part of the larger challenge in detecting LLM-generated text.  \n",
      "\n",
      "4. **Conclusion**:  \n",
      "   These parts highlight the **inaccuracy and bias** in LLM-generated content, reinforcing the need for careful detection to mitigate issues like information misrepresentation.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Get the actual text from the response object\n",
    "response_text = response[\"response\"]  # or response.response if it's an attribute\n",
    "\n",
    "# Use regex to capture everything after </think>\n",
    "match = re.search(r\"</think>\\s*(.*)\", response_text, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    extracted_text = match.group(1).strip()\n",
    "else:\n",
    "    extracted_text = \"\"\n",
    "\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2668e971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let me tackle this step by step. First, I need to identify the sentences or paragraphs from both the query and the matched PDF that are similar. Then, I should check why they are similar and explain that briefly. Finally, write the explanation in clear, natural language.\n",
      "\n",
      "Starting with the query page: The user is comparing two different pages. In the query, there's a section about LLM-generated text detection. Then the matched PDF has a similar section about detecting fake news and bias. So that's two similar parts.\n",
      "\n",
      "Next, the explanation. The first part is highlighting the similarity between the query and the PDF's similar sections. The user needs to explain why these parts are similar. The query mentions LLM-generated text detection, and the PDF has similar content about detecting fake news and ensuring bias. So the reason is that both are about ensuring the content is accurate and avoiding issues like bias or fake news.\n",
      "\n",
      "Then, the explanation should be concise. Maybe mention that both sections are about the same problem areas and that the PDF explains those areas in detail. Finally, the user wants the explanation in clear, natural language, so no markdown and just plain text.\n",
      "</think>\n",
      "\n",
      "1. **Highlight sentences from the query and matched PDF:**  \n",
      "   - **Query Page:** \"LLM-generated text detection\" and \"DetectGPT's performance metrics.\"  \n",
      "   - **PDF Matched Section:** \"DetectGPT's detection rate\" and \"watermarking schemes.\"  \n",
      "\n",
      "2. **Show corresponding sentences from the PDF:**  \n",
      "   - \"LLM-generated text detection\" and \"DetectGPT's detection rate.\"  \n",
      "\n",
      "3. **Explanation:**  \n",
      "   The query and PDF share similarities in discussing **LLM-generated text detection** and **how to detect biases or fake content**. Both sections focus on ensuring accuracy, with the query emphasizing detection methods and the PDF detailing performance metrics like detection rate and watermarking. This similarity highlights the common goal of safeguarding LLM-generated content from errors or malicious inputs.  \n",
      "\n",
      "4. **Explanation in clear, natural language:**  \n",
      "   The query and PDF both discuss **LLM-generated text detection** and how to ensure accuracy or detect biases. The query highlights detection methods, while the PDF explains performance metrics like detection rate and watermarking. This shows that the two sections are about the same problem area, emphasizing the importance of accuracy and detection in LLM-generated text.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_page_text(pdf_path, page_number):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    if page_number < len(doc):\n",
    "        text = doc[page_number].get_text(\"text\")\n",
    "    else:\n",
    "        text = \"\"\n",
    "    doc.close()\n",
    "    return text.strip()\n",
    "\n",
    "query_pdf = extract_page_text(query_pdf_filepath, 1)      # page 1\n",
    "matched_pdf = extract_page_text(\"/home/natcha/rag_document/dataset/1-s2.0-S2949719125000275-main.pdf\", 3)\n",
    "similarity_percentage = 53.91\n",
    "\n",
    "# Run a prompt on qwen3:0.6b-q4_K_M\n",
    "response = ollama.generate(\n",
    "    model=\"qwen3:0.6b-q4_K_M\",\n",
    "    prompt=f\"\"\"\n",
    "Compare the following two texts and explain why they are considered {similarity_percentage}% similar. \n",
    "Highlight common keywords, phrases, or ideas. \n",
    "\n",
    "Here is the query page text:\n",
    "---\n",
    "{query_pdf}\n",
    "---\n",
    "\n",
    "Here is the matched database page text:\n",
    "---\n",
    "{matched_pdf}\n",
    "---\n",
    "\n",
    "Tasks:\n",
    "1. Highlight sentences or paragraphs from the query page that are similar.  \n",
    "2. Show the corresponding sentences or paragraphs from the matched PDF.  \n",
    "3. Explain briefly why these parts are considered similar.  \n",
    "4. Write the explanation in clear, natural language.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4e3ddb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle this task. The user wants me to highlight sentences or paragraphs from the query page that are similar to those in the matched PDF. Then, I need to show the corresponding sentences from both and explain why they are similar.\n",
      "\n",
      "First, I need to look at the query and the matched PDF. Let me start with the query. The user mentioned two main tasks: highlighting similar sentences or paragraphs and showing them from both documents. Then, explaining the reasons.\n",
      "\n",
      "For the query, I'll check for key points. The query talks about LLM-generated text detection, mentioning zero-shot and watermark detectors. In the matched PDF, there's also a section about paraphrasing attacks. So, both documents have information about detection methods, which are similar. That's one area.\n",
      "\n",
      "Next, the query mentions accuracy issues in LLM-generated text. In the matched PDF, there's a part about information inaccuracy. So, these are similar. The query's point about accuracy leading to misleading users is mentioned, while the PDF discusses the risk of inaccuracies. Both are about the same issue, so they are similar.\n",
      "\n",
      "I need to make sure I highlight the exact sentences from both documents. The query has \"With the massive creation of LLM-generated content on the web, there is a risk that future training of LLMs could include LLM-generated content without necessarily knowing...\" and in the matched PDF, there's a similar sentence. Then explain why they are similar by noting that both discuss the same problem.\n",
      "\n",
      "Also, check if there are other similar sentences. The query mentions detection performance varying with text length and decoding strategies, which the PDF also mentions. That's another similarity.\n",
      "\n",
      "Finally, structure the response by first highlighting the first two sentences, then showing them from both documents, and explaining why they are similar. Make sure the explanation is brief but clear.\n",
      "</think>\n",
      "\n",
      "1. **Highlight sentences from both documents**:  \n",
      "   - **Query**: \"With the massive creation of LLM-generated content on the web, there is a risk that future training of LLMs could include LLM-generated content without necessarily knowing...\"  \n",
      "   - **PDF**: \"With the massive creation of LLM-generated content on the web, there is a risk that future training of LLMs could include LLM-generated content without necessarily knowing...\"  \n",
      "\n",
      "   **Explanation**: Both documents discuss the risk of future training including LLM-generated content without awareness, highlighting the same concern.  \n",
      "\n",
      "2. **Show corresponding sentences**:  \n",
      "   - **Query**: \"With the massive creation of LLM-generated content on the web, there is a risk that future training of LLMs could include LLM-generated content without necessarily knowing...\"  \n",
      "   - **PDF**: \"With the massive creation of LLM-generated content on the web, there is a risk that future training of LLMs could include LLM-generated content without necessarily knowing...\"  \n",
      "\n",
      "   **Explanation**: These sentences share the same focus on the risk of future training, making them similar in explanation.  \n",
      "\n",
      "3. **Explain similarity**:  \n",
      "   Both documents emphasize the same issue (risk of future training including LLM-generated content) and its consequences (accuracy issues).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_page_text(pdf_path, page_number):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    if page_number < len(doc):\n",
    "        text = doc[page_number].get_text(\"text\")\n",
    "    else:\n",
    "        text = \"\"\n",
    "    doc.close()\n",
    "    return text.strip()\n",
    "\n",
    "query_pdf = extract_page_text(query_pdf_filepath, 1)      # page 1\n",
    "matched_pdf = extract_page_text(\"/home/natcha/rag_document/dataset/1-s2.0-S2949719125000275-main.pdf\", 3)\n",
    "similarity_percentage = 53.91\n",
    "\n",
    "# Run a prompt on qwen3:0.6b-q4_K_M\n",
    "response = ollama.generate(\n",
    "    model=\"qwen3:0.6b-q4_K_M\",\n",
    "    prompt=f\"\"\"\n",
    "You are comparing two PDF pages. \n",
    "Your goal is to compare the following two texts and explain why they are considered {similarity_percentage}% similar. \n",
    "Highlight common keywords, phrases, or ideas. \n",
    "\n",
    "Here is the query page text:\n",
    "---\n",
    "{query_pdf}\n",
    "---\n",
    "\n",
    "Here is the matched database page text:\n",
    "---\n",
    "{matched_pdf}\n",
    "---\n",
    "\n",
    "Tasks:\n",
    "1. Highlight sentences or paragraphs from the query page that are similar.  \n",
    "2. Show the corresponding sentences or paragraphs from the matched PDF.  \n",
    "3. Explain briefly why these parts are considered similar.  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "302740f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let me start by understanding the user's request. They want me to identify and highlight overlapping sentences, key terms, or concepts in both provided texts. Then, present the matching parts side by side and explain the similarity in meaning, wording, or ideas.\n",
      "\n",
      "First, I need to look at both texts. Let me read through them again. The first text is about detecting fake news and bias in LLM models. The second text is about LLM-generated text detection methods, like zero-shot and watermark detectors. \n",
      "\n",
      "Looking at the first text, there's mention of bias, like ethical concerns from LLM sampling. In the second text, there's also a part about bias in training data, such as the Redditor's data. So overlapping terms here are bias and training data.\n",
      "\n",
      "Next, key terms: in the first text, zero-shot and watermark detectors are mentioned. In the second text, they also talk about zero-shot and watermark detectors. So the overlapping terms are zero-shot and watermark detectors.\n",
      "\n",
      "For the matching parts, I need to see where the two texts have similar parts. For example, both texts mention detection performance metrics like AUROC. Also, both texts discuss how detecting performance varies with text length and decoding strategies. That's a good overlap.\n",
      "\n",
      "In terms of explanation, the similarity in meaning is that both texts discuss detection methods. The wording is similar in terms of using terms like AUROC and watermarking. Ideas are about detecting LLM-generated text and addressing issues like bias and inaccurate information.\n",
      "\n",
      "I need to make sure all the points are covered clearly. Let me check again for any other overlaps. Also, ensure that the explanation is concise and highlights the key points.\n",
      "</think>\n",
      "\n",
      "**1. Overlapping Sentences/Key Terms**  \n",
      "- **Bias**: Both texts discuss ethical concerns related to LLM sampling (first text: \"ethical concerns... bias\"), and training data biases (second text: \"bias in training...\").\n",
      "- **Detection Methods**: Both texts mention detection methods like zero-shot and watermark detectors (first text: \"zero-shot classifiers... detectors,\" second text: \"zero-shot classifiers... detectors\").\n",
      "\n",
      "**2. Matching Parts**  \n",
      "- **AUROC Metrics**: Both texts include metrics like AUROC (first text: \"DetectGPT... AUROC...\"), and second text: \"watermark detection... AUROC (Kirchenbauer et al., 2023).\"  \n",
      "- **Watermarking**: Both texts reference watermarking schemes (first text: \"watermarking scheme... green list,\" second text: \"watermarking... green list tokens\").  \n",
      "\n",
      "**3. Similarity in Meaning/Wording**  \n",
      "- **Core Idea**: Both texts focus on addressing issues like bias and inaccurate information in LLM-generated content. The emphasis is on **detecting LLM-generated text** and mitigating problems like fake news and gender bias. The overlap lies in **methods for detecting LLM-generated text** and **mitigating bias/accuracy issues**.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(\n",
    "    model=\"qwen3:0.6b-q4_K_M\",\n",
    "    prompt=f\"\"\"\n",
    "You are analyzing two pieces of text from different PDF pages.  \n",
    "Your task is to evaluate why they are considered {similarity_percentage}% similar.  \n",
    "\n",
    "Here is the first (query) page:\n",
    "---\n",
    "{query_pdf}\n",
    "---\n",
    "\n",
    "Here is the second (matched database) page:\n",
    "---\n",
    "{matched_pdf}\n",
    "---\n",
    "\n",
    "Tasks:\n",
    "1. Identify and highlight overlapping sentences, key terms, or concepts.  \n",
    "2. Present the matching parts from both texts side by side.  \n",
    "3. Provide a short explanation describing the similarity in meaning, wording, or ideas.  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80da3bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let me try to figure out why the two PDF texts are labeled as 53.91% similar. First, I need to look at the tasks given. The user wants me to quote specific phrases or sentences that overlap, show the corresponding passages from both texts, and summarize the reason for similarity. \n",
      "\n",
      "Starting with the first task, the user wants specific phrases or sentences. Let me check the two texts. In the first text, there's a mention of \"Ethical concerns\" and \"Bias\" related to gender, race, and ethnicity. In the second text, there's \"Information inaccuracy\" and \"Bias\" again. That's a lot of overlap. \n",
      "\n",
      "Then, the second task is to show the corresponding passages. I'll find the relevant parts. In the first text, \"Ethical concerns. Ethical considerations concerning gender, race and ethnicities bias are raised attributing to the inherent sampling bias of the LLM, which potentially leads to social inequality and discrimination (An et al., 2024).\" In the second text, \"Ethical concerns. Ethical considerations concerning gender, race and ethnicities bias are raised attributing to the inherent sampling bias of the LLM, which potentially leads to social inequality and discrimination (An et al., 2024).\" So those are the same phrases. \n",
      "\n",
      "For the third task, the summary. The overlap is in terms of ethical concerns and bias, which are related to the training data's inherent biases. Both texts discuss similar topics, so the reason is that they highlight the ethical and bias issues in LLM training, which contribute to their similarity. \n",
      "\n",
      "Wait, but the user wants the summary to be in the context of the two texts. So, the reason is that both texts discuss similar concepts, such as ethical considerations and bias in LLM training, which are overlapping in the context of their content. \n",
      "\n",
      "I need to make sure I'm not missing any other overlapping points. The first text talks about the training data's bias leading to issues, and the second also mentions similar points. That's the main overlap. So the summary should mention that both texts discuss similar topics related to ethical and bias issues in LLM training.\n",
      "</think>\n",
      "\n",
      "1. **Overlapping Phrases/Sentences**:  \n",
      "   - **First Text**: \"Ethical concerns. Ethical considerations concerning gender, race and ethnicities bias are raised attributing to the inherent sampling bias of the LLM, which potentially leads to social inequality and discrimination (An et al., 2024).\"  \n",
      "   - **Second Text**: \"Ethical concerns. Ethical considerations concerning gender, race and ethnicities bias are raised attributing to the inherent sampling bias of the LLM, which potentially leads to social inequality and discrimination (An et al., 2024).\"  \n",
      "\n",
      "2. **Corresponding Passages**:  \n",
      "   - **First Text**: The ethical concerns and bias issues are discussed in the context of training data sampling bias, leading to social inequality.  \n",
      "   - **Second Text**: Similarly, the same ethical and bias-related topics are highlighted in the context of LLM training and its implications.  \n",
      "\n",
      "3. **Reason for Similarity**:  \n",
      "   Both texts focus on ethical and bias-related issues in LLM training, which are central to their content. The overlap arises from the shared focus on the ethical and sampling bias problems in training models, contributing to a high similarity in their analysis.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(\n",
    "    model=\"qwen3:0.6b-q4_K_M\",\n",
    "    prompt=f\"\"\"\n",
    "You are given two PDF page texts.  \n",
    "Your job is to determine why they are labeled as {similarity_percentage}% similar.  \n",
    "\n",
    "Query page text:\n",
    "---\n",
    "{query_pdf}\n",
    "---\n",
    "\n",
    "Matched PDF text:\n",
    "---\n",
    "{matched_pdf}\n",
    "---\n",
    "\n",
    "Tasks:\n",
    "1. Quote the specific phrases or sentences that overlap.  \n",
    "2. Show the corresponding passages from both texts.  \n",
    "3. Summarize the reason for similarity (keywords, context, or shared meaning).  \n",
    "\"\"\"\n",
    ")\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82e86a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle these tasks. First, task 1 is to extract the most similar sentences/paragraphs from the query text. The query text is a bit long, so I need to look through it carefully. Let me start by reading through it again to identify key sentences. The user mentioned \"most similar\", so I need to find sentences that have similar content or structure. Maybe the first few paragraphs or the main points of the query. Let me check the query text again. The first part talks about detecting LLM-generated text, then the database text discusses issues like information inaccuracy. The query's main topic is about LLM-generated text detection. So the most similar sentences might be the first few paragraphs of the query text. I need to make sure I capture those.\n",
      "\n",
      "Now, task 2 is to show the parallel passages from the database text. The database text has two main parts: the detection methods and the issues with LLMs. The first part is about zero-shot and watermark detectors. The second part is about the problems with accuracy and training. So the database text has parallel sentences in each section. I need to find the overlapping sentences in both texts. For example, in the query, the first few paragraphs are about detecting LLM-generated text. In the database, the first part is about the detectors, then the issues. The overlap here would be the general topic of detecting LLM-generated text. So I need to identify the parallel sections in both texts and show them.\n",
      "\n",
      "For task 3, explain the overlap in terms of wording, subject matter, or structure. The query text has a focus on detection methods. The database text has sections on both detectors and their performance. The overlap would be the common elements like detection methods and performance metrics. I need to look for similar words or phrases in both texts. For example, the query mentions \"zero-shot classifiers\" and \"watermark detectors\", while the database mentions \"zero-shot\" and \"watermarking\". The structure is similar in terms of presenting detection methods. So I need to explain how these elements overlap.\n",
      "\n",
      "Now, putting it all together. First, in the query, the main topic is LLM-generated text detection. The first few paragraphs discuss the methods. In the database, the first part is about detectors, then the issues. The overlap is in the detection methods and performance aspects. So the explanation would be about how the detection methods and performance metrics are presented in both texts.\n",
      "</think>\n",
      "\n",
      "1. **Extract the most similar sentences/paragraphs from the query text**:  \n",
      "   The query text emphasizes detection methods like zero-shot and watermark-based detectors. Key sentences include:  \n",
      "   - \"Zero-shot classifiers aim to identify patterns and statistical characteristics of input text, comparing them to those of LLM-generated text\" (Solaiman et al., 2019).  \n",
      "   - \"Watermark detectors rely on the addition of a watermark, which is not visible to humans, on LLM-generated text.\" (Kirchenbauer et al., 2023).  \n",
      "\n",
      "2. **Show the parallel passages from the database text**:  \n",
      "   In the database, the first part discusses zero-shot and watermark-based detectors, while the second section highlights issues like information inaccuracy. The overlap is in the detection methods and performance metrics:  \n",
      "   - \"Zero-shot classifiers do not require further training, thus facilitating usage from non-technical users.\" (Solaiman et al., 2019).  \n",
      "   - \"Watermark detectors are more effective than zero-shot classifiers.\" (Krishna et al., 2024).  \n",
      "\n",
      "3. **Explain the overlap in terms of wording, subject matter, or structure**:  \n",
      "   The overlap lies in the focus on detection methods (zero-shot and watermark-based) and performance metrics (accuracy, AUROC). Both texts present similar concepts, with the query text highlighting the mechanics of detection, and the database text emphasizing the effectiveness of these methods.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(\n",
    "    model=\"qwen3:0.6b-q4_K_M\",\n",
    "    prompt=f\"\"\"\n",
    "Compare the following two PDF page texts and justify why their similarity score is {similarity_percentage}%.  \n",
    "\n",
    "Query page:\n",
    "---\n",
    "{query_pdf}\n",
    "---\n",
    "\n",
    "Database page:\n",
    "---\n",
    "{matched_pdf}\n",
    "---\n",
    "\n",
    "Tasks:\n",
    "1. Extract the most similar sentences/paragraphs from the query text.  \n",
    "2. Show the parallel passages from the database text.  \n",
    "3. Explain the overlap in terms of wording, subject matter, or structure.  \n",
    "\"\"\"\n",
    ")\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba67345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's start by looking at the query and the matched text. \n",
      "\n",
      "First, Task 1 is to identify parts of the query text that align closely with the matched text. I need to check where both documents have similar content. \n",
      "\n",
      "Looking at the query, there's a mention of LLM-generated text detection. In the query, there's a section about detecting text using watermarks, which is similar to the matched text. The query also talks about zero-shot classifiers, which is mentioned in the matched text. Then there's a section about paraphrasing attacks, which is mentioned in both. \n",
      "\n",
      "So, the query has parts that match the matched text. The matched text also covers similar topics. Now, Task 2 is to provide the corresponding parts. \n",
      "\n",
      "For Task 3, I need to explain why these sections are similar. The shared terminology includes LLM-generated text detection, zero-shot classifiers, and paraphrasing attacks. The topics are all related to text detection methods. \n",
      "\n",
      "I need to make sure I'm not missing any parts. Let me check again. Yes, the query and matched text both discuss similar topics. The explanation should highlight the shared terms and topics.\n",
      "</think>\n",
      "\n",
      "1. **Identify parts of the query text that align closely with the matched text:**  \n",
      "   - The query text discusses **LLM-generated text detection** using **watermarking** and **zero-shot classifiers**.  \n",
      "   - The matched text also mentions similar topics, such as **text detection methods** and **paraphrasing attacks**.  \n",
      "\n",
      "2. **Provide the corresponding parts from the matched text:**  \n",
      "   - Query: \"With the massive creation of LLM-generated content on the web, there is a risk that future training of LLMs could include LLM-generated content without necessarily knowing...\"  \n",
      "   - Matched text: \"With the massive creation of LLM-generated content on the web, there is a risk that future training of LLMs could include LLM-generated content without necessarily knowing...\"  \n",
      "\n",
      "3. **Explain why these sections are similar (shared terminology, topics, or ideas):**  \n",
      "   - Both documents discuss **text detection methods** such as **watermarking**, **zero-shot classifiers**, and **paraphrasing attacks**. The shared terminology and topics highlight the focus on **LLM-generated text detection** and its related security and accuracy challenges.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(\n",
    "    model=\"qwen3:0.6b-q4_K_M\",\n",
    "    prompt=f\"\"\"\n",
    "Your role is to act as a similarity evaluator.  \n",
    "You will be given two PDF texts and a similarity score of {similarity_percentage}%.  \n",
    "\n",
    "Query text:\n",
    "---\n",
    "{query_pdf}\n",
    "---\n",
    "\n",
    "Matched text:\n",
    "---\n",
    "{matched_pdf}\n",
    "---\n",
    "\n",
    "Tasks:\n",
    "1. Identify parts of the query text that align closely with the matched text.  \n",
    "2. Provide the corresponding parts from the matched text.  \n",
    "3. Explain briefly why these sections are similar (shared terminology, topics, or ideas).  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba67f453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle this task. The user wants me to highlight sentences/paragraphs from the query that match closely with the PDF, show the equivalent sentences/paragraphs, and explain why they're similar.\n",
      "\n",
      "First, I need to look at both the query and the PDF. The query has some text, and the PDF has similar content. I need to find the exact sentences or paragraphs that are similar.\n",
      "\n",
      "Looking at the query, there's a mention of \"zero-shot classifiers\" and \"watermark detectors\". In the query, they say \"zero-shot classifiers\" and \"watermark detectors\". In the PDF, they also reference these terms. So that's a match.\n",
      "\n",
      "Next, the query talks about \"information accuracy\" and \"inaccuracy\". In the PDF, there's a similar part about \"information accuracy\" and \"inaccuracy\". So that's another match.\n",
      "\n",
      "Then, the query mentions \"LLM-generated text detection\" and \"watermark detection\". In the PDF, they have \"watermark detection\" as a term. So that's another match.\n",
      "\n",
      "So the three matches are the terms \"zero-shot classifiers\", \"watermark detectors\", and \"information accuracy\". The explanations need to state why these parts are similar. For the first match, the query and PDF both refer to zero-shot and watermark detectors. The PDF explains their use and performance. For the second, the query and PDF both discuss information accuracy issues. The PDF explains the problem. Finally, the query and PDF mention LLM-generated text detection, and the PDF explains the approach.\n",
      "\n",
      "Each of these matches should be highlighted, and the explanations should explain why they're similar. I need to make sure the sentences are accurately translated from the original text and that the explanations clearly state the similarity.\n",
      "</think>\n",
      "\n",
      "1. **Query & PDF Match:**  \n",
      "   - **Query:** \"zero-shot classifiers\" and \"watermark detectors\" are mentioned in both documents.  \n",
      "   - **PDF:** The same terms are present, with explanations about their functionality and performance.  \n",
      "   - **Similarity:** Both documents discuss detection methods, emphasizing their role in identifying LLM-generated content.  \n",
      "\n",
      "2. **Query & PDF Match:**  \n",
      "   - **Query:** \"information accuracy\" and \"inaccuracy\" are referenced.  \n",
      "   - **PDF:** The same terms are used, explaining how LLMs may produce inaccurate outputs due to biased training data.  \n",
      "   - **Similarity:** Both documents address the issue of inaccuracies in LLM-generated text, highlighting the problem's relevance.  \n",
      "\n",
      "3. **Query & PDF Match:**  \n",
      "   - **Query:** \"LLM-generated text detection\" and \"watermark detection\" are mentioned.  \n",
      "   - **PDF:** The same terms are explained, focusing on how watermarking and classification techniques detect LLM-generated content.  \n",
      "   - **Similarity:** Both documents discuss the detection methodology, showing close alignment in the context of LLM-generated text monitoring.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(\n",
    "    model=\"qwen3:0.6b-q4_K_M\",\n",
    "    prompt=f\"\"\"\n",
    "You are tasked with reviewing two PDF texts to explain their similarity score of {similarity_percentage}%.  \n",
    "\n",
    "Here is the query page content:\n",
    "---\n",
    "{query_pdf}\n",
    "---\n",
    "\n",
    "Here is the matched PDF content:\n",
    "---\n",
    "{matched_pdf}\n",
    "---\n",
    "\n",
    "Tasks:\n",
    "1. Highlight sentences/paragraphs from the query that match closely.  \n",
    "2. Show the equivalent sentences/paragraphs from the matched PDF.  \n",
    "3. Write a short explanation of why these parts are considered similar.  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7c3cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let me try to figure out how these two PDF pages are similar. The user wants me to summarize why they are considered similar and provide an example of a sentence they share. \n",
      "\n",
      "First, I'll look at the content of both pages. The first one talks about LLM-generated text detection, mentioning zero-shot and watermark detectors. The second page also discusses similar topics, but with different details. \n",
      "\n",
      "I need to find common elements. Both pages mention detection methods like zero-shot and watermarking. In the first, they use RoBERTa and GROVER for accuracy. In the second, they mention detection performance varying with text length and decoding strategies. That's a common point. Also, both pages discuss the vulnerabilities of these methods, like paraphrasing attacks.\n",
      "\n",
      "Now, the example sentence. Let me check. In the first, they talk about detecting GPT-2 with a watermark. In the second, they mention using DIPPER AI-paraphraser. Both involve detecting text with specific methods. So the common example is detecting LLM-generated text using either approach. That's a solid example.\n",
      "</think>\n",
      "\n",
      "The two PDF pages share similarities in their focus on **LLM-generated text detection** and **vulnerabilities**. Both pages discuss detection methods such as **zero-shot classifiers** and **watermarking detectors**, which are used to identify text as LLM-generated. Here's an example of a common sentence:\n",
      "\n",
      "**\"DetectGPT calculates the average log probability ratio of the input text over its perturbations and classifies text as LLM-generated if the ratio exceeds a threshold.\"**\n",
      "\n",
      "This sentence reflects the core mechanisms of both pages, which are designed to detect LLM-generated content by analyzing patterns or watermarks.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_page_text(pdf_path, page_number):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    if page_number < len(doc):\n",
    "        text = doc[page_number].get_text(\"text\")\n",
    "    else:\n",
    "        text = \"\"\n",
    "    doc.close()\n",
    "    return text.strip()\n",
    "\n",
    "query_pdf = extract_page_text(query_pdf_filepath, 1)      # page 1\n",
    "matched_pdf = extract_page_text(\"/home/natcha/rag_document/dataset/1-s2.0-S2949719125000275-main.pdf\", 3)\n",
    "similarity_percentage = 53.91\n",
    "\n",
    "# Run a prompt on qwen3:0.6b-q4_K_M\n",
    "response = ollama.generate(\n",
    "    model=\"qwen3:0.6b-q4_K_M\",\n",
    "    prompt=f\"\"\"\n",
    "You are comparing two PDF pages. \n",
    "Your goal is to explain why they are considered {similarity_percentage}% similar.\n",
    "\n",
    "Here is the query page text:\n",
    "---\n",
    "{query_pdf}\n",
    "---\n",
    "\n",
    "Here is the matched database page text:\n",
    "---\n",
    "{matched_pdf}\n",
    "---\n",
    "\n",
    "Tasks:\n",
    "summarize what make these 2 pdf pages similar with show example of the sentence thtay have in commons\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(response[\"response\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
